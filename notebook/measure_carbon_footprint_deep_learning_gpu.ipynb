{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8e0c57-8acf-4848-9145-e546ea6b6ed3",
   "metadata": {},
   "source": [
    "# Example of a Deep learning Pipleine using CodeCarbon & CarbonTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e51c79-7906-410e-9b06-c7031d216e5b",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf8b45-7a24-4668-9a37-b904559b82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "logger = logging.getLogger(\"logger\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2.dsl import Artifact, Input, Metrics, Model, Output, component, Dataset, ClassificationMetrics\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eee668-461e-43cf-b109-99da2cf85004",
   "metadata": {},
   "source": [
    "## Define global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2babeab-1ca7-444e-a0e1-774cdfa58f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"europe-west4\" #Netherlands region\n",
    "\n",
    "# Get projet name\n",
    "shell_output=!gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "\n",
    "BUCKET_NAME = \"test\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline_root_mist/\"\n",
    "\n",
    "# Give the name of the pipeline\n",
    "PIPELINE_JSON_NAME = 'pipeline-deep-learning-codecarbon.json'\n",
    "USER_FLAG = \"--user\"\n",
    "\n",
    "# Give the experiment name\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "TASK = \"classif\"\n",
    "MODEL_TYPE = \"gpu-cnn\"\n",
    "\n",
    "# Set the name of the pipeline job\n",
    "DISPLAY_NAME = 'pipeline-deep-learning-cnn-gpu-job{}'.format(TIMESTAMP)\n",
    "\n",
    "# If you need a service account \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"../../secrets/test.json\"\n",
    "SA=\"test@YOUR_PROJECT_ID.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2d9b9-7bb5-4124-9188-d1d23477d547",
   "metadata": {},
   "source": [
    "## Authenticate your Google Cloud account\n",
    "\n",
    "In the Cloud Console, go to the Create service account key page.\n",
    "* **Click** Create service account.\n",
    "* In the Service account name field, enter a name, and click Create.\n",
    "* In the Grant this service account access to project section, click the Role drop-down list. Type \"Vertex AI\" into the filter box, and select Vertex AI Administrator. Type \"Storage Object Admin\" into the filter box, and select Storage Object Admin.\n",
    "* Click Create. A JSON file that contains your key downloads to your local environment.\n",
    "* Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf43a6c-1bcc-42b0-ab3b-bddb8ffdd29a",
   "metadata": {},
   "source": [
    "## Create a custom deep learning component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "210fd311-802a-4a6c-9b66-7bd4b6dfbb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"codecarbon\",\n",
    "        \"carbontracker\",\n",
    "    ], base_image=\"gcr.io/deeplearning-platform-release/tf-gpu.2-11:latest\",\n",
    ")\n",
    "def train_deep_learning_fashion_mist_job(  \n",
    "    learning_rate: float, \n",
    "    epochs: int,\n",
    "    batch_size: int,\n",
    "    kpi_co2: Output[Metrics],\n",
    "    model_trained : Output[Model],\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import os   \n",
    "    import pandas as pd\n",
    "    #Import Code Carbon\n",
    "    from codecarbon import  OfflineEmissionsTracker   \n",
    "    # Import CarbonTracker\n",
    "    from carbontracker.tracker import CarbonTracker\n",
    "    from carbontracker import parser\n",
    "    \n",
    "    #Define variables \n",
    "    NO_CLASSES = 10\n",
    "    img_rows, img_cols = 28, 28\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    DIR_LOG='log' #dir to track carbon tracker logs \n",
    "\n",
    "    #Fashion-MNIST is a dataset of Zalando's article \n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    " \n",
    "    def create_dir(log):\n",
    "        try:\n",
    "            os.makedirs(log)\n",
    "        except OSError:\n",
    "            pass  \n",
    "    create_dir(DIR_LOG)\n",
    "    \n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()    \n",
    "       \n",
    "\n",
    "    #recale training images \n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /=  255.0\n",
    "    x_test /=  255.0\n",
    "\n",
    "    model=tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Dense(128,activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128,activation='relu'),\n",
    "        tf.keras.layers.Dense(NO_CLASSES,activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    #Start Codecarbon and specify The Netherlands acronyms NLD\n",
    "    codecarbon = OfflineEmissionsTracker(country_iso_code=\"NLD\")\n",
    "    codecarbon.start()\n",
    "    \n",
    "    #Start Carbon traker \n",
    "    carbon_tracker = CarbonTracker(epochs=epochs, log_dir=\"./\"+DIR_LOG+\"/\")   \n",
    "    carbon_tracker.epoch_start()\n",
    "    \n",
    "    #Fit model on training data\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2)\n",
    "    \n",
    "    #Stop carbon tracking\n",
    "    carbontracker.epoch_end()\n",
    "    emissions_codecarbon: float = codecarbon.stop()\n",
    "    logs = parser.parse_all_logs(log_dir=\"./\"+DIR_LOG+\"/\")\n",
    "    carbontracker.stop()\n",
    "    \n",
    "    #Log data\n",
    "    first_log = logs[0]\n",
    "    emissions_carbontracker: dict = first_log['pred']\n",
    "    kpi_co2.log_metric(\"emissions_codecarbon co2eq (g/kwh)\", float(emissions_codecarbon) * 1000)\n",
    "    kpi_co2.log_metric(\"emissions_carbontraker co2eq (g/kwh)\", emissions_carbontracker['co2eq (g)'])\n",
    "    # Save model \n",
    "    model.save(model_trained.path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5dba138-ad37-4a63-ab80-537468bd8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"gcr.io/deeplearning-platform-release/tf-gpu.2-11:latest\")\n",
    "def evaluate_model(\n",
    "    model_trained: Input[Model], \n",
    "    algo_metrics: Output[Metrics], \n",
    "    metric_confusion_matrix: Output[ClassificationMetrics]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    img_rows, img_cols = 28, 28\n",
    "    \n",
    "    # Get test data \n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (_, _), (x_test, y_test) = fashion_mnist.load_data() \n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_test /=  255.0\n",
    "\n",
    "    #Load the model\n",
    "    model = tf.keras.models.load_model(model_trained.path)\n",
    "    #Generate predictions on test dataset \n",
    "    predictions = model.predict(x_test).argmax(1)\n",
    "    # Evaluate the model\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size=64)\n",
    " \n",
    "    # Log metrics\n",
    "    algo_metrics.log_metric(\"accuracy\", acc * 100)\n",
    "    algo_metrics.log_metric(\"loss\", loss)\n",
    "    metric_confusion_matrix.log_confusion_matrix([str(i) for i in range(10)], confusion_matrix(y_test, predictions).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ab3b1-f955-41be-8835-b899383adaff",
   "metadata": {},
   "source": [
    "## Create the Pipeline\n",
    "### The pipeline compilation generates the **pipeline-deep-learning-codecarbon.json**  job spec file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "097c767b-7683-4e78-9ccd-625581ca2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-deep-learning\",\n",
    ")\n",
    "def pipeline(\n",
    "    learning_rate: float, \n",
    "    epochs: int, \n",
    "    batch_size: int, \n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",  \n",
    "    serving_container_image_uri: str = \"europe-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-11:latest\"\n",
    "    ): \n",
    "    \n",
    "    deep_learning_mist_task = (\n",
    "        train_deep_learning_fashion_mist_job(learning_rate=learning_rate, epochs=epochs, batch_size=batch_size)\n",
    "        .add_node_selector_constraint(\n",
    "            label_name=\"cloud.google.com/gke-accelerator\",\n",
    "            value=\"NVIDIA_TESLA_T4\")\n",
    "        .set_gpu_limit(1))\n",
    "    \n",
    "    model_evaluation_task = evaluate_model(deep_learning_mist_task.outputs['model_trained'])\n",
    "\n",
    "#Compile the pipeline\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=PIPELINE_JSON_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98bd92-a27b-4af1-979a-24d2c9ad44e2",
   "metadata": {},
   "source": [
    "## Submit the pipeline job in an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439fe90-5886-42c8-9aa7-888c2bb76559",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_conf_params = {\"learning_rate\": 0.001, \"epochs\": 10, \"batch_size\": 64}\n",
    "job = aiplatform.PipelineJob(\n",
    "        display_name=f\"{DISPLAY_NAME}-pipeline-run\",\n",
    "        template_path=PIPELINE_JSON_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        parameter_values={\n",
    "            **run_conf_params,\n",
    "        },\n",
    "        enable_caching=False,\n",
    "        location=REGION,\n",
    "    )\n",
    "job.submit(service_account=SA)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "vertex_env",
   "name": "tf2-gpu.2-9.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m103"
  },
  "kernelspec": {
   "display_name": "vertex_env",
   "language": "python",
   "name": "vertex_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
